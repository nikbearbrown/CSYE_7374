{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Visualization — Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Latent Space Visualization \n",
    "\n",
    "Deep learning and neural networks are increasingly important concepts as demonstrated through their performance on difficult problems in computer vision, medical diagnosis, natural language processing and many other domains. Deep learning algorithms are unique in that they try to learn latent features from data, as opposed to traditional machine learning where features selection is typically handcrafted. However, the semantics of deep neural networks “hidden layers” are poorly understood, and are often treated as “black box” models.  The aim of _Latent Space Visualization_ is to develop tools and algorithms to better understand the semantics of the latent features learned by deep networks, particularly those used for unsupervised deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the latent space?  \n",
    "\n",
    "The word “latent” means “hidden”. In a neural networks some of the data is in the space that you can observe, and other data is within “hidden layers.”  \n",
    "\n",
    "In _latent Dirichlet allocation (LDA)_ the \"latent space\" is the vector space within which the vectors that make up the topics found by LDA are found.\n",
    "\n",
    "In both cases, the \"latent space\" allows sets of observations to be mapped by unobserved groups that explain why some parts of the data are similar. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What numbers exist in the latent space? \n",
    "\n",
    "The values in the “hidden layers” are typically numerical weights usually represented as vectors. This means we can plot, calculate distance and similarlity and use any matheimatical technique especially those that work with vectors on the latent space.\n",
    "\n",
    "\n",
    "The basic idea behind latent space visualization is to map numbers in the _hidden layers_ whose values are usually just weights to output that has meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  t-SNE visualization\n",
    "\n",
    "### What’s t-SNE?\n",
    "   \n",
    "**t-distributed stochastic neighbor embedding (t-SNE)** is a machine\n",
    "learning algorithm for dimensionality reduction developed by\n",
    "Geoffrey Hinton and Laurens van der Maaten. It is a nonlinear\n",
    "dimensionality reduction technique that is particularly well-suited for\n",
    "embedding high-dimensional data into a space of two or three dimensions,\n",
    "which can then be visualized in a scatter plot. Specifically, it\n",
    "models each high-dimensional object by a two- or three-dimensional point\n",
    "in such a way that similar objects are modeled by nearby points and\n",
    "dissimilar objects are modeled by distant points.\n",
    "\n",
    "The t-SNE algorithm comprises two main stages. \n",
    "\n",
    "**Step 1**\n",
    "\n",
    "First, t-SNE constructs a\n",
    "probability distribution over pairs of high-dimensional objects in\n",
    "such a way that similar objects have a high probability of being picked,\n",
    "whilst dissimilar points have an extremely small probability of being\n",
    "picked. \n",
    "\n",
    "**Step 2**  \n",
    "\n",
    "Second, t-SNE defines a similar probability distribution over\n",
    "the points in the low-dimensional map, and it minimizes the\n",
    "Kullback–Leibler divergence between the two distributions with respect\n",
    "to the locations of the points in the map. Note that whilst the original\n",
    "algorithm uses the Euclidean distance between objects as the base of\n",
    "its similarity metric, this should be changed as appropriate.\n",
    "\n",
    "**Kullback–Leibler divergence**  \n",
    "\n",
    "For discrete probability distributions *P* and *Q*, the\n",
    "Kullback–Leibler divergence from *Q* to *P* is defined^1 to be\n",
    "\n",
    "$$D_{\\mathrm{KL}}(P\\|Q) = \\sum_i P(i) \\, \\log\\frac{P(i)}{Q(i)}.$$\n",
    "\n",
    "In other words, it is the expectation of the logarithmic difference\n",
    "between the probabilities *P* and *Q*, where the expectation is taken\n",
    "using the probabilities *P*. The Kullback–Leibler divergence is defined\n",
    "only if *Q*(*i*)=0 implies *P*(*i*)=0, for all *i* (absolute\n",
    "continuity). Whenever *P*(*i*) is zero the contribution of the *i*-th\n",
    "term is interpreted as zero because $\\lim_{x \\to 0} x \\log(x) = 0$.\n",
    "\n",
    "For distributions *P* and *Q* of a continuous random variable, the\n",
    "Kullback–Leibler divergence is defined to be the integral:  \n",
    "\n",
    "$$D_{\\mathrm{KL}}(P\\|Q) = \\int_{-\\infty}^\\infty p(x) \\, \\log\\frac{p(x)}{q(x)} \\, dx,$$   \n",
    "\n",
    "where *p* and *q* denote the densities of *P* and *Q*.\n",
    "\n",
    "More generally, if *P* and *Q* are probability measures over a set\n",
    "*X*, and *P* is absolutely continuous with respect to *Q*, then the\n",
    "Kullback–Leibler divergence from *Q* to *P* is defined as\n",
    "\n",
    "$$D_{\\mathrm{KL}}(P\\|Q) = \\int_X \\log\\frac{dP}{dQ} \\, dP,$$\n",
    "\n",
    "where $\\frac{dP}{dQ}$ is the Radon–Nikodym derivative of *P* with\n",
    "respect to *Q*, and provided the expression on the right-hand side\n",
    "exists. Equivalently, this can be written as\n",
    "\n",
    "$$D_{\\mathrm{KL}}(P\\|Q) = \\int_X \\log\\!\\left(\\frac{dP}{dQ}\\right) \\frac{dP}{dQ} \\, dQ,$$\n",
    "\n",
    "which we recognize as the entropy of *P* relative to *Q*. Continuing\n",
    "in this case, if $\\mu$ is any measure on *X* for which\n",
    "$p = \\frac{dP}{d\\mu}$ and $q = \\frac{dQ}{d\\mu}$ exist (meaning that *p*\n",
    "and *q* are absolutely continuous with respect to $\\mu$), then the\n",
    "Kullback–Leibler divergence from *Q* to *P* is given as\n",
    "\n",
    "$$D_{\\mathrm{KL}}(P\\|Q) = \\int_X p \\, \\log \\frac{p}{q} \\, d\\mu.\n",
    "\\!$$\n",
    "\n",
    "The logarithms in these formulae are taken to base 2 if information is\n",
    "measured in units of bits, or to base *e* if information is measured\n",
    "in nats. Most formulas involving the Kullback–Leibler divergence hold\n",
    "regardless of the base of the logarithm.\n",
    "\n",
    "Various conventions exist for referring to *D*~KL~(*P*‖*Q*) in words.\n",
    "Often it is referred to as the divergence *between* *P* and *Q*; however\n",
    "this fails to convey the fundamental asymmetry in the relation.\n",
    "Sometimes, as in this article, it may be found described as the\n",
    "divergence of *P* from, or with respect to *Q*. This reflects the\n",
    "asymmetry in Bayesian inference, which starts *from* a prior *Q* and\n",
    "updates *to* the posterior *P*.  \n",
    "\n",
    "### t-SNE algorithm\n",
    "\n",
    "Given a set of $N$ high-dimensional objects\n",
    "$\\mathbf{x}_1, \\dots, \\mathbf{x}_N$, t-SNE first computes probabilities\n",
    "$p_{ij}$ that are proportional to the similarity of objects\n",
    "$\\mathbf{x}_i$ and $\\mathbf{x}_j$, as follows:   \n",
    "\n",
    " $$p_{j\\mid i} = \\frac{\\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\lVert\\mathbf{x}_i - \\mathbf{x}_k\\rVert^2 / 2\\sigma_i^2)},$$\n",
    "\n",
    "As Van der Maaten and Hinton explained : “The similarity of datapoint\n",
    "$x_j$ to datapoint $x_i$ is the conditional probability, $p_{j|i}$, that\n",
    "$x_i$ would pick $x_j$ as its neighbor if neighbors were picked in\n",
    "proportion to their probability density under a Gaussian centered at\n",
    "$x_i$.”  \n",
    "\n",
    " $$p_{ij} = \\frac{p_{j\\mid i} + p_{i\\mid j}}{2N}$$\n",
    " \n",
    " The bandwidth of the [Gaussian kernels] $\\sigma_i$, is set in such a way\n",
    "that the [perplexity] of the conditional distribution equals a\n",
    "predefined perplexity using the [bisection method]. As a result, the\n",
    "bandwidth is adapted to the [density] of the data: smaller values of\n",
    "$\\sigma_i$ are used in denser parts of the data space.\n",
    "\n",
    "t-SNE aims to learn a $d$-dimensional map\n",
    "$\\mathbf{y}_1, \\dots, \\mathbf{y}_N$ (with\n",
    "$\\mathbf{y}_i \\in \\mathbb{R}^d$) that reflects the similarities $p_{ij}$\n",
    "as well as possible. To this end, it measures similarities $q_{ij}$\n",
    "between two points in the map $\\mathbf{y}_i$ and $\\mathbf{y}_j$, using a\n",
    "very similar approach. Specifically, $q_{ij}$ is defined as:   \n",
    "\n",
    "   $$q_{ij} = \\frac{(1 + \\lVert \\mathbf{y}_i - \\mathbf{y}_j\\rVert^2)^{-1}}{\\sum_{k \\neq m} (1 + \\lVert \\mathbf{y}_k - \\mathbf{y}_m\\rVert^2)^{-1}}$$\n",
    "\n",
    "Herein a heavy-tailed [Student-t distribution] (with one-degree of\n",
    "freedom, which is the same as a [Cauchy distribution]) is used to\n",
    "measure similarities between low-dimensional points in order to allow\n",
    "dissimilar objects to be modeled far apart in the map.\n",
    "\n",
    "The locations of the points $\\mathbf{y}_i$ in the map are determined by\n",
    "minimizing the (non-symmetric) [Kullback–Leibler divergence] of the\n",
    "distribution $Q$ from the distribution $P$, that is:   \n",
    "\n",
    "   $$KL(P||Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "The minimization of the Kullback–Leibler divergence with respect to the\n",
    "points $\\mathbf{y}_i$ is performed using [gradient descent]. The result\n",
    "of this optimization is a map that reflects the similarities between the\n",
    "high-dimensional inputs well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import model_from_json\n",
    "import cv2 # pip install opencv-python\n",
    "import sys\n",
    "import os\n",
    "from random import shuffle, randint, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates random image with squares and circles\n",
    "def getRandomImage():\n",
    "\timageSize = 100\n",
    "\tsize = 25\n",
    "\tnbShapes = 5\n",
    "\n",
    "\txy = lambda: randint(0,100)\n",
    "\n",
    "\t# Create a white image\n",
    "\timg = np.zeros((imageSize,imageSize,3), np.uint8)\n",
    "\tcv2.rectangle(img,(0,0),(imageSize,imageSize),(122,122,122) ,-1)\n",
    "\n",
    "\tgreyImg = np.copy(img)\n",
    "\n",
    "\t# Adds some shapes\n",
    "\tfor i in range(nbShapes):\n",
    "\t\tx0, y0 = xy(), xy()\n",
    "\t\tisRect = choice((True,False))\n",
    "\t\tif isRect:\n",
    "\t\t\tcv2.rectangle(img,(x0,y0),(x0+size,y0+size),(255,0,0) ,-1)\n",
    "\t\t\tcv2.rectangle(greyImg,(x0,y0),(x0+size,y0+size),(255,255,255) ,-1)\n",
    "\t\telse:\n",
    "\t\t\tcv2.circle(img,(x0,y0), size/2, (0,0,255), -1)\n",
    "\t\t\tcv2.circle(greyImg,(x0,y0), size/2, (255,255,255), -1)\n",
    "\n",
    "\treturn cv2.resize(img,(48,48)), cv2.resize(greyImg,(48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the dataset\n",
    "def getDataset(display=False):\n",
    "\t# Show what the dataset looks like\n",
    "\tif display:\n",
    "\t\tcolorImg, greyImg = getRandomImage()\n",
    "\t\timg = np.hstack((colorImg, greyImg))\n",
    "\t\tcv2.imshow(\"Dataset\",cv2.resize(img,(200,100)))\n",
    "\t\tcv2.waitKey(0)\n",
    "\t\tcv2.destroyAllWindows()\n",
    "\n",
    "\t#for i in range\n",
    "\n",
    "\tx_train, x_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "\t# Add training examples\n",
    "\tfor i in range(10000):\n",
    "\t\tcolorImg, greyImg = getRandomImage()\n",
    "\t\tgreyImg = cv2.cvtColor(greyImg, cv2.COLOR_RGB2GRAY)\n",
    "\t\tx_train.append(greyImg.astype('float32')/255.)\n",
    "\t\ty_train.append(colorImg.astype('float32')/255.)\n",
    "\n",
    "\t# Add test examples\n",
    "\tfor i in range(1000):\n",
    "\t\tcolorImg, greyImg = getRandomImage()\n",
    "\t\tgreyImg = cv2.cvtColor(greyImg, cv2.COLOR_RGB2GRAY)\n",
    "\t\tx_test.append(greyImg.astype('float32')/255.)\n",
    "\t\ty_test.append(colorImg.astype('float32')/255.)\n",
    "\n",
    "\t# Reshape\n",
    "\tx_train = np.array(x_train).reshape((-1,48,48,1))\n",
    "\tx_test = np.array(x_test).reshape((-1,48,48,1))\n",
    "\ty_train = np.array(y_train).reshape((-1,48,48,3))\n",
    "\ty_test = np.array(y_test).reshape((-1,48,48,3))\n",
    "\n",
    "\treturn x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the Convolutional Auto Encoder\n",
    "def getModel():\n",
    "\tinput_img = Input(shape=(48, 48, 1))\n",
    "\tx = Convolution2D(16, 3, 3, activation='relu', border_mode='same', dim_ordering='tf')(input_img)\n",
    "\tx = MaxPooling2D((2, 2), border_mode='same', dim_ordering='tf')(x)\n",
    "\tx = Convolution2D(32, 3, 3, activation='relu', border_mode='same', dim_ordering='tf')(input_img)\n",
    "\tx = MaxPooling2D((2, 2), border_mode='same', dim_ordering='tf')(x)\n",
    "\tx = Convolution2D(64, 3, 3, activation='relu', border_mode='same', dim_ordering='tf')(x)\n",
    "\tencoded = MaxPooling2D((2, 2), border_mode='same', dim_ordering='tf')(x)\n",
    "\t#6x6x32 -- bottleneck\n",
    "\tx = UpSampling2D((2, 2), dim_ordering='tf')(encoded)\n",
    "\tx = Convolution2D(32, 3, 3, activation='relu', border_mode='same', dim_ordering='tf')(x)\n",
    "\tx = UpSampling2D((2, 2), dim_ordering='tf')(x)\n",
    "\tx = Convolution2D(16, 3, 3, activation='relu', border_mode='same', dim_ordering='tf')(x)\n",
    "\tdecoded = Convolution2D(3, 3, 3, activation='relu', border_mode='same', dim_ordering='tf')(x)\n",
    "\n",
    "\t#Create model\n",
    "\tautoencoder = Model(input_img, decoded)\n",
    "\treturn autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trains the model for 10 epochs\n",
    "def trainModel():\n",
    "\t# Load dataset\n",
    "\tprint(\"Loading dataset...\")\n",
    "\tx_train_gray, x_train, x_test_gray, x_test = getDataset()\n",
    "\n",
    "\t# Create model description\n",
    "\tprint(\"Creating model...\")\n",
    "\tmodel = getModel()\n",
    "\tmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\t# Train model\n",
    "\tprint(\"Training model...\")\n",
    "\tmodel.fit(x_train_gray, x_train, nb_epoch=10, batch_size=148, shuffle=True, validation_data=(x_test_gray, x_test), callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])\n",
    "\n",
    "\t# Evaluate loaded model on test data\n",
    "\tprint(\"Evaluating model...\")\n",
    "\tscore = model.evaluate(x_train_gray, x_train, verbose=0)\n",
    "\tprint (\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "\t# Serialize model to JSON\n",
    "\tprint(\"Saving model...\")\n",
    "\tmodel_json = model.to_json()\n",
    "\twith open(\"model.json\", \"w\") as json_file:\n",
    "\t    json_file.write(model_json)\n",
    "\t    \n",
    "\t# Serialize weights to HDF5\n",
    "\tprint(\"Saving weights...\")\n",
    "\tmodel.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tests the model and shows results\n",
    "def testModel():\n",
    "\t# Load JSON model description\n",
    "\twith open('model.json', 'r') as json_file:\n",
    "\t\tmodelJSON = json_file.read()\n",
    "\t\n",
    "\t# Build model from JSON description\n",
    "\tprint(\"Loading model...\")\n",
    "\tmodel = model_from_json(modelJSON)\n",
    "\n",
    "\t# Load weights\n",
    "\tprint(\"Loading weights...\")\n",
    "\tmodel.load_weights(\"model.h5\")\n",
    "\n",
    "\t_, _, x_test_gray, x_test = getDataset()\n",
    "\tx_test_gray = x_test_gray[:10]\n",
    "\tx_test = x_test[:10]\n",
    "\n",
    "\tprint(\"Making predictions...\")\n",
    "\tpredictions = model.predict(x_test_gray)\n",
    "\tx_test_gray = [cv2.cvtColor(img,cv2.COLOR_GRAY2RGB) for img in x_test_gray]\n",
    "\n",
    "\timg = np.vstack((np.hstack(x_test_gray),np.hstack(predictions),np.hstack(x_test)))\n",
    "\n",
    "\tcv2.imshow(\"Input - Reconstructed - Ground truth\",cv2.resize(img,(img.shape[1],img.shape[0])))\n",
    "\tcv2.waitKey(0)\n",
    "\tcv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data\n",
    "\n",
    "The [MNIST database](http://yann.lecun.com/exdb/mnist/) of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEIj/B11a1SNwfNbiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129134828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# plot first 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  t-SNE Latent Space Visualization\n",
    "\n",
    "\n",
    "### t-SNE MNIST visualizations\n",
    "\n",
    "This example is from [Latent space visualization — Deep Learning bits #2](\n",
    "https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df)\n",
    "\n",
    "\n",
    "**t-SNE visualization of MNIST validation set**\n",
    "\n",
    "![t-SNE visualization of MNIST validation set](http://nikbearbrown.com/YouTube/MachineLearning/IMG/t-SNE_visualization_of_MNIST_validation_set.png) \n",
    "\n",
    "* t-SNE projection of image space representations from the validation set   \n",
    "### MNIST autoencoder reconstruction\n",
    "\n",
    "![MNIST autoencoder reconstruction](http://nikbearbrown.com/YouTube/MachineLearning/IMG/MNIST_autoencoder_reconstruction.png)\n",
    "\n",
    "The autoencoder successfully reconstructs the digits. The reconstruction is blurry because the input is compressed.\n",
    "\n",
    "![MNIST autoencoder reconstruction amimation](http://nikbearbrown.com/YouTube/MachineLearning/IMG/MNIST_autoencoder_reconstruction_animation.gif)\n",
    "\n",
    "\n",
    "**t-SNE projection of latent space representations from the MNIST validation set**\n",
    "\n",
    "![t-SNE projection of latent space representations from the MNIST validation set](http://nikbearbrown.com/YouTube/MachineLearning/IMG/t-SNE_projection_of_latent_space_representations_from_the_MNIST_validation_set.png)   \n",
    "\n",
    "* t-SNE projection of latent space representations from the MNIST validation set  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Latent space visualization — Deep Learning bits #2\n",
    "\n",
    "Code for Latent space visualization — Deep Learning bits #2 is at [https://github.com/despoisj/LatentSpaceVisualization](https://github.com/despoisj/LatentSpaceVisualization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from scipy.stats import norm\n",
    "from sklearn import manifold\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, UpSampling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "\n",
    "# Config\n",
    "\n",
    "modelsPath = \"Models/\"\n",
    "visualsPath = \"Visuals/\"\n",
    "\n",
    "imageSize = 28\n",
    "latent_dim = 32\n",
    "\n",
    "# Convolutional models\n",
    "# x is input, z is \n",
    "def getModels():\n",
    "    input_img = Input(shape=(imageSize, imageSize, 1))\n",
    "    x = Convolution2D(32, 3, 3, border_mode='same')(input_img)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "\n",
    "    x = Convolution2D(64, 3, 3, border_mode='same')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "\n",
    "    # Latent space // bottleneck layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(latent_dim)(x)\n",
    "    z = ELU()(x)\n",
    "\n",
    "    ##### MODEL 1: ENCODER #####\n",
    "    encoder = Model(input_img, z)\n",
    "   \n",
    "    # We instantiate these layers separately so as to reuse them for the decoder\n",
    "    # Dense from latent space to image dimension\n",
    "    x_decoded_dense1 = Dense(7 * 7 * 64)\n",
    "\n",
    "    # Reshape for image\n",
    "    x_decoded_reshape0 = Reshape((7, 7, 64))\n",
    "    x_decoded_upsample0 = UpSampling2D((2, 2))\n",
    "    x_decoded_conv0  = Convolution2D(32, 3, 3, border_mode='same')\n",
    "\n",
    "    x_decoded_upsample3 = UpSampling2D((2, 2))\n",
    "    x_decoded_conv3 = Convolution2D(1, 3, 3, activation='sigmoid', border_mode='same')\n",
    "\n",
    "    # Create second part of autoencoder\n",
    "    x_decoded = x_decoded_dense1(z)\n",
    "    x_decoded = ELU()(x_decoded)\n",
    "\n",
    "    x_decoded = x_decoded_reshape0(x_decoded)\n",
    "    x_decoded = x_decoded_upsample0(x_decoded)\n",
    "    x_decoded = x_decoded_conv0(x_decoded)\n",
    "    x_decoded = ELU()(x_decoded)\n",
    "\n",
    "    # Tanh layer\n",
    "    x_decoded = x_decoded_upsample3(x_decoded)\n",
    "    decoded_img = x_decoded_conv3(x_decoded)\n",
    "\n",
    "    ##### MODEL 2: AUTO-ENCODER #####\n",
    "    autoencoder = Model(input_img, decoded_img)\n",
    "\n",
    "    # Create decoder\n",
    "    input_z = Input(shape=(latent_dim,))\n",
    "    x_decoded_decoder = x_decoded_dense1(input_z)\n",
    "    x_decoded_decoder = ELU()(x_decoded_decoder)\n",
    "\n",
    "    x_decoded_decoder = x_decoded_reshape0(x_decoded_decoder)\n",
    "    x_decoded_decoder = x_decoded_upsample0(x_decoded_decoder)\n",
    "    x_decoded_decoder = x_decoded_conv0(x_decoded_decoder)\n",
    "    x_decoded_decoder = ELU()(x_decoded_decoder)\n",
    "\n",
    "    # Tanh layer\n",
    "    x_decoded_decoder = x_decoded_upsample3(x_decoded_decoder)\n",
    "    decoded_decoder_img = x_decoded_conv3(x_decoded_decoder)\n",
    "\n",
    "    ##### MODEL 3: DECODER #####\n",
    "    decoder = Model(input_z, decoded_decoder_img)\n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from scipy.stats import norm\n",
    "from sklearn import manifold\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, UpSampling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "\n",
    "# Convolutional models\n",
    "# x is input, z is \n",
    "def getModels():\n",
    "    input_img = Input(shape=(imageSize, imageSize, 1))\n",
    "    x = Convolution2D(32, 3, 3, border_mode='same')(input_img)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "\n",
    "    x = Convolution2D(64, 3, 3, border_mode='same')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "\n",
    "    # Latent space // bottleneck layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(latent_dim)(x)\n",
    "    z = ELU()(x)\n",
    "\n",
    "    ##### MODEL 1: ENCODER #####\n",
    "    encoder = Model(input_img, z)\n",
    "   \n",
    "    # We instantiate these layers separately so as to reuse them for the decoder\n",
    "    # Dense from latent space to image dimension\n",
    "    x_decoded_dense1 = Dense(7 * 7 * 64)\n",
    "\n",
    "    # Reshape for image\n",
    "    x_decoded_reshape0 = Reshape((7, 7, 64))\n",
    "    x_decoded_upsample0 = UpSampling2D((2, 2))\n",
    "    x_decoded_conv0  = Convolution2D(32, 3, 3, border_mode='same')\n",
    "\n",
    "    x_decoded_upsample3 = UpSampling2D((2, 2))\n",
    "    x_decoded_conv3 = Convolution2D(1, 3, 3, activation='sigmoid', border_mode='same')\n",
    "\n",
    "    # Create second part of autoencoder\n",
    "    x_decoded = x_decoded_dense1(z)\n",
    "    x_decoded = ELU()(x_decoded)\n",
    "\n",
    "    x_decoded = x_decoded_reshape0(x_decoded)\n",
    "    x_decoded = x_decoded_upsample0(x_decoded)\n",
    "    x_decoded = x_decoded_conv0(x_decoded)\n",
    "    x_decoded = ELU()(x_decoded)\n",
    "\n",
    "    # Tanh layer\n",
    "    x_decoded = x_decoded_upsample3(x_decoded)\n",
    "    decoded_img = x_decoded_conv3(x_decoded)\n",
    "\n",
    "    ##### MODEL 2: AUTO-ENCODER #####\n",
    "    autoencoder = Model(input_img, decoded_img)\n",
    "\n",
    "    # Create decoder\n",
    "    input_z = Input(shape=(latent_dim,))\n",
    "    x_decoded_decoder = x_decoded_dense1(input_z)\n",
    "    x_decoded_decoder = ELU()(x_decoded_decoder)\n",
    "\n",
    "    x_decoded_decoder = x_decoded_reshape0(x_decoded_decoder)\n",
    "    x_decoded_decoder = x_decoded_upsample0(x_decoded_decoder)\n",
    "    x_decoded_decoder = x_decoded_conv0(x_decoded_decoder)\n",
    "    x_decoded_decoder = ELU()(x_decoded_decoder)\n",
    "\n",
    "    # Tanh layer\n",
    "    x_decoded_decoder = x_decoded_upsample3(x_decoded_decoder)\n",
    "    decoded_decoder_img = x_decoded_conv3(x_decoded_decoder)\n",
    "\n",
    "    ##### MODEL 3: DECODER #####\n",
    "    decoder = Model(input_z, decoded_decoder_img)\n",
    "    return autoencoder, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads the dataset - here Normalized MNIST\n",
    "def loadDataset():\n",
    "    from keras.datasets import mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape([-1,28,28,1])/255.\n",
    "    X_test = X_test.reshape([-1,28,28,1])/255.\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import cv2\n",
    "import math\n",
    "import random, string\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "\n",
    "# Show every image, good for picking interplation candidates\n",
    "def visualizeDataset(X):\n",
    "    for i,image in enumerate(X):\n",
    "        cv2.imshow(str(i),image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Scatter with images instead of points\n",
    "def imscatter(x, y, ax, imageData, zoom):\n",
    "    images = []\n",
    "    for i in range(len(x)):\n",
    "        x0, y0 = x[i], y[i]\n",
    "        # Convert to image\n",
    "        img = imageData[i]*255.\n",
    "        img = img.astype(np.uint8).reshape([imageSize,imageSize])\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "        # Note: OpenCV uses BGR and plt uses RGB\n",
    "        image = OffsetImage(img, zoom=zoom)\n",
    "        ab = AnnotationBbox(image, (x0, y0), xycoords='data', frameon=False)\n",
    "        images.append(ax.add_artist(ab))\n",
    "    \n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "\n",
    "# Show dataset images with T-sne projection of latent space encoding\n",
    "def computeTSNEProjectionOfLatentSpace(X, encoder, display=True):\n",
    "    # Compute latent space representation\n",
    "    print(\"Computing latent space projection...\")\n",
    "    X_encoded = encoder.predict(X)\n",
    "\n",
    "    # Compute t-SNE embedding of latent space\n",
    "    print(\"Computing t-SNE embedding...\")\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    X_tsne = tsne.fit_transform(X_encoded)\n",
    "\n",
    "    # Plot images according to t-sne embedding\n",
    "    if display:\n",
    "        print(\"Plotting t-SNE visualization...\")\n",
    "        fig, ax = plt.subplots()\n",
    "        imscatter(X_tsne[:, 0], X_tsne[:, 1], imageData=X, ax=ax, zoom=0.6)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return X_tsne\n",
    "\n",
    "# Show dataset images with T-sne projection of pixel space\n",
    "def computeTSNEProjectionOfPixelSpace(X, display=True):\n",
    "    # Compute t-SNE embedding of latent space\n",
    "    print(\"Computing t-SNE embedding...\")\n",
    "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "    X_tsne = tsne.fit_transform(X.reshape([-1,imageSize*imageSize*1]))\n",
    "\n",
    "    # Plot images according to t-sne embedding\n",
    "    if display:\n",
    "        print(\"Plotting t-SNE visualization...\")\n",
    "        fig, ax = plt.subplots()\n",
    "        imscatter(X_tsne[:, 0], X_tsne[:, 1], imageData=X, ax=ax, zoom=0.6)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return X_tsne\n",
    "\n",
    "# Reconstructions for samples in dataset\n",
    "def getReconstructedImages(X, autoencoder):\n",
    "    nbSamples = X.shape[0]\n",
    "    nbSquares = int(math.sqrt(nbSamples))\n",
    "    nbSquaresHeight = 2*nbSquares\n",
    "    nbSquaresWidth = nbSquaresHeight\n",
    "    resultImage = np.zeros((nbSquaresHeight*imageSize,nbSquaresWidth*imageSize/2,X.shape[-1]))\n",
    "\n",
    "    reconstructedX = autoencoder.predict(X)\n",
    "\n",
    "    for i in range(nbSamples):\n",
    "        original = X[i]\n",
    "        reconstruction = reconstructedX[i]\n",
    "        rowIndex = i%nbSquaresWidth\n",
    "        columnIndex = (i-rowIndex)/nbSquaresHeight\n",
    "        resultImage[rowIndex*imageSize:(rowIndex+1)*imageSize,columnIndex*2*imageSize:(columnIndex+1)*2*imageSize,:] = np.hstack([original,reconstruction])\n",
    "\n",
    "    return resultImage\n",
    "\n",
    "# Reconstructions for samples in dataset\n",
    "def visualizeReconstructedImages(X_train, X_test, autoencoder, save=False, label=False):\n",
    "    trainReconstruction = getReconstructedImages(X_train,autoencoder)\n",
    "    testReconstruction = getReconstructedImages(X_test,autoencoder)\n",
    "\n",
    "    if not save:\n",
    "        print(\"Generating 10 image reconstructions...\")\n",
    "\n",
    "    result = np.hstack([trainReconstruction,np.zeros([trainReconstruction.shape[0],5,trainReconstruction.shape[-1]]),testReconstruction])\n",
    "    result = (result*255.).astype(np.uint8)\n",
    "\n",
    "    if save:\n",
    "        cv2.imwrite(visualsPath+\"reconstructions_{}.png\".format(label),result)\n",
    "    else:\n",
    "        cv2.imshow(\"Reconstructed images (train - test)\",result)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Computes A, B, C, A+B, A+B-C in latent space\n",
    "def visualizeArithmetics(a, b, c, encoder, decoder):\n",
    "    print(\"Computing arithmetics...\")\n",
    "    # Create micro batch\n",
    "    X = np.array([a,b,c])\n",
    "\n",
    "    # Compute latent space projection\n",
    "    latentA, latentB, latentC = encoder.predict(X)\n",
    "\n",
    "    add = latentA+latentB\n",
    "    addSub = latentA+latentB-latentC\n",
    "\n",
    "    # Create micro batch\n",
    "    X = np.array([latentA,latentB,latentC,add,addSub])\n",
    "\n",
    "    # Compute reconstruction\n",
    "    reconstructedA, reconstructedB, reconstructedC, reconstructedAdd, reconstructedAddSub = decoder.predict(X)\n",
    "\n",
    "    cv2.imshow(\"Arithmetics in latent space\",np.hstack([reconstructedA, reconstructedB, reconstructedC, reconstructedAdd, reconstructedAddSub]))\n",
    "    cv2.waitKey()\n",
    "\n",
    "# Shows linear inteprolation in image space vs latent space\n",
    "def visualizeInterpolation(start, end, encoder, decoder, save=False, nbSteps=5):\n",
    "    print(\"Generating interpolations...\")\n",
    "\n",
    "    # Create micro batch\n",
    "    X = np.array([start,end])\n",
    "\n",
    "    # Compute latent space projection\n",
    "    latentX = encoder.predict(X)\n",
    "    latentStart, latentEnd = latentX\n",
    "\n",
    "    # Get original image for comparison\n",
    "    startImage, endImage = X\n",
    "\n",
    "    vectors = []\n",
    "    normalImages = []\n",
    "    #Linear interpolation\n",
    "    alphaValues = np.linspace(0, 1, nbSteps)\n",
    "    for alpha in alphaValues:\n",
    "        # Latent space interpolation\n",
    "        vector = latentStart*(1-alpha) + latentEnd*alpha\n",
    "        vectors.append(vector)\n",
    "        # Image space interpolation\n",
    "        blendImage = cv2.addWeighted(startImage,1-alpha,endImage,alpha,0)\n",
    "        normalImages.append(blendImage)\n",
    "\n",
    "    # Decode latent space vectors\n",
    "    vectors = np.array(vectors)\n",
    "    reconstructions = decoder.predict(vectors)\n",
    "\n",
    "    # Put final image together\n",
    "    resultLatent = None\n",
    "    resultImage = None\n",
    "\n",
    "    if save:\n",
    "        hashName = ''.join(random.choice(string.lowercase) for i in range(3))\n",
    "\n",
    "    for i in range(len(reconstructions)):\n",
    "        interpolatedImage = normalImages[i]*255\n",
    "        interpolatedImage = cv2.resize(interpolatedImage,(50,50))\n",
    "        interpolatedImage = interpolatedImage.astype(np.uint8)\n",
    "        resultImage = interpolatedImage if resultImage is None else np.hstack([resultImage,interpolatedImage])\n",
    "\n",
    "        reconstructedImage = reconstructions[i]*255.\n",
    "        reconstructedImage = reconstructedImage.reshape([28,28])\n",
    "        reconstructedImage = cv2.resize(reconstructedImage,(50,50))\n",
    "        reconstructedImage = reconstructedImage.astype(np.uint8)\n",
    "        resultLatent = reconstructedImage if resultLatent is None else np.hstack([resultLatent,reconstructedImage])\n",
    "    \n",
    "        if save:\n",
    "            cv2.imwrite(visualsPath+\"{}_{}.png\".format(hashName,i),np.hstack([interpolatedImage,reconstructedImage]))\n",
    "\n",
    "        result = np.vstack([resultImage,resultLatent])\n",
    "\n",
    "    if not save:\n",
    "        cv2.imshow(\"Interpolation in Image Space vs Latent Space\",result)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Features of a Convolutional Neural Network\n",
    "\n",
    "Matworks MATLAB docs describe how to [Visualize Features of a Convolutional Neural Network](https://www.mathworks.com/help/nnet/examples/visualize-features-of-a-convolutional-neural-network.html)    \n",
    "\n",
    "and how to [Visualize Activations of a Convolutional Neural Network](https://www.mathworks.com/help/nnet/examples/visualize-activations-of-a-convolutional-neural-network.html)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE visualization of CNN codes\n",
    "\n",
    "@karpathy took 50,000 ILSVRC 2012 validation images, extracted the 4096-dimensional fc7 CNN (Convolutional Neural Network) features using Caffe and then used Barnes-Hut t-SNE to compute a 2-dimensional embedding that respects the high-dimensional (L2) distances. In other words, t-SNE arranges images that have a similar CNN (fc7) code nearby in the embedding.\n",
    "\n",
    "\n",
    "'fc7'      Fully Connected               4096 fully connected layer\n",
    "\n",
    "Not sure of his mapping?\n",
    "\n",
    "\n",
    "From [ImageNet Classification with Deep Convolutional\n",
    "Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)  \n",
    "\n",
    "\n",
    "\"Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\n",
    "results for many more test images in the supplementary material.\"\n",
    "\n",
    "![ImageNet Classification with Deep Convolutional Neural Networks Figure 5](http://nikbearbrown.com/YouTube/MachineLearning/IMG/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks_Fig_5.png)\n",
    "\n",
    "* ImageNet Classification with Deep Convolutional Neural Networks Figure 5  \n",
    "\n",
    "\n",
    "See [t-SNE visualization of CNN codes](http://cs.stanford.edu/people/karpathy/cnnembed/)  \n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg) \n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_4k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_4k.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_4k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_4k.jpg)  \n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_6k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_6k.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_6k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_6k.jpg)  \n",
    "\n",
    "\n",
    "And below, embeddings where every position is filled with its nearest neighbor. Note that since the actual embedding is roughly circular, this leads to a visualization where the corners are a little \"stretched\" out and over-represented:  \n",
    "\n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg)\n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k.jpg)  \n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_6k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_6k.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_6k.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_6k.jpg)  \n",
    "\n",
    "\n",
    "**final visualization**  \n",
    "\n",
    "![http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k_seams.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k_seams.jpg)  \n",
    "\n",
    "[http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k_seams.jpg](http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k_seams.jpg)  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear interpolation in latent space\n",
    "\n",
    "A linear interpolation (LERP) takes two vectors and an alpha value and returns a new vector that represents the interpolation between the two input vectors. \n",
    "\n",
    "![LERP](http://nikbearbrown.com/YouTube/MachineLearning/IMG/LERP.png)\n",
    "\n",
    "\n",
    "See LERP - Linear Interpolation [https://youtu.be/0MHkgPqc-P4](https://youtu.be/0MHkgPqc-P4)\n",
    "\n",
    "## Linear interpolation in MNIST latent space\n",
    "\n",
    "We take the same start and end images and feed them to the encoder to obtain their latent space representation. We then interpolate between the two latent vectors, and feed these to the decoder.\n",
    "\n",
    "![LERP](http://nikbearbrown.com/YouTube/MachineLearning/IMG/LERP.png)\n",
    "\n",
    "![LERP Two Three](http://nikbearbrown.com/YouTube/MachineLearning/IMG/LERP_Two_Three.png)\n",
    "\n",
    "![LERP Five Seven](http://nikbearbrown.com/YouTube/MachineLearning/IMG/LERP_Five_Seven.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space arithmetics\n",
    "\n",
    "We can also do arithmetics in the latent space. This means that instead of interpolating, we can add or subtract latent space representations.\n",
    "\n",
    "### Vector addition\n",
    "\n",
    "![Vector addition and scalar multiplication](http://nikbearbrown.com/YouTube/MachineLearning/IMG/530px-Vector_add_scale.svg.png)\n",
    "\n",
    "* Vector addition and scalar multiplication: a vector v (blue) is added to another vector w (red, upper illustration). Below, w is stretched by a factor of 2, yielding the sum v + 2w.   \n",
    "\n",
    "\n",
    "For example with faces, man with glasses - man without glasses + woman without glasses = woman with glasses. This technique gives mind-blowing results.\n",
    "\n",
    "![Latent_space_arithmetics.gif](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Latent_space_arithmetics.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Latent Space of Vector Drawings from the Google QuickDraw Dataset with SketchRNN, PCA and t-SNE\n",
    "\n",
    "Visualizing the Latent Space of Vector Drawings from the Google QuickDraw Dataset with SketchRNN, PCA and t-SNE [http://louistiao.me/posts/notebooks/visualizing-the-latent-space-of-vector-drawings-from-the-google-quickdraw-dataset-with-sketchrnn-pca-and-t-sne/](http://louistiao.me/posts/notebooks/visualizing-the-latent-space-of-vector-drawings-from-the-google-quickdraw-dataset-with-sketchrnn-pca-and-t-sne/)   \n",
    "\n",
    "\n",
    "![SketchRNN](http://nikbearbrown.com/YouTube/MachineLearning/IMG/SketchRNN.png)\n",
    "\n",
    "\n",
    "![SketchRNN cluster](http://nikbearbrown.com/YouTube/MachineLearning/IMG/SketchRNN_cluster.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update October 3, 2017 \n",
    "\n",
    "The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
